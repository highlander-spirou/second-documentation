"use strict";(self.webpackChunkcode_docs=self.webpackChunkcode_docs||[]).push([[53],{1109:a=>{a.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Pyspark","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Install Pyspark","href":"/second-documentation/docs/pyspark/Install pyspark","docId":"pyspark/Install pyspark"},{"type":"link","label":"M\xf4 h\xecnh ho\u1ea1t \u0111\u1ed9ng c\u1ee7a pyspark","href":"/second-documentation/docs/pyspark/M\xf4 h\xecnh Spark","docId":"pyspark/M\xf4 h\xecnh Spark"},{"type":"link","label":"RDD v\xe0 DataFrame","href":"/second-documentation/docs/pyspark/RDD v\xe0 DataFrame","docId":"pyspark/RDD v\xe0 DataFrame"},{"type":"link","label":"Quy t\u1eafc x\u1eed l\xed d\u1eef li\u1ec7u","href":"/second-documentation/docs/pyspark/Quy t\u1eafc x\u1eed l\xed d\u1eef li\u1ec7u","docId":"pyspark/Quy t\u1eafc x\u1eed l\xed d\u1eef li\u1ec7u"},{"type":"category","label":"Data Analysis","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Datetime functions","href":"/second-documentation/docs/pyspark/Data Analysis/Datetime","docId":"pyspark/Data Analysis/Datetime"},{"type":"link","label":"Explode row","href":"/second-documentation/docs/pyspark/Data Analysis/Explode row","docId":"pyspark/Data Analysis/Explode row"},{"type":"link","label":"Case when","href":"/second-documentation/docs/pyspark/Data Analysis/case when","docId":"pyspark/Data Analysis/case when"},{"type":"link","label":"Concat (stack) dataframes","href":"/second-documentation/docs/pyspark/Data Analysis/concat","docId":"pyspark/Data Analysis/concat"},{"type":"link","label":"Groupby and Aggregation","href":"/second-documentation/docs/pyspark/Data Analysis/groupby and agg","docId":"pyspark/Data Analysis/groupby and agg"},{"type":"link","label":"Join","href":"/second-documentation/docs/pyspark/Data Analysis/join","docId":"pyspark/Data Analysis/join"},{"type":"link","label":"Melt dataframe","href":"/second-documentation/docs/pyspark/Data Analysis/melt","docId":"pyspark/Data Analysis/melt"},{"type":"link","label":"User defined function","href":"/second-documentation/docs/pyspark/Data Analysis/udf","docId":"pyspark/Data Analysis/udf"},{"type":"link","label":"Window function","href":"/second-documentation/docs/pyspark/Data Analysis/window function","docId":"pyspark/Data Analysis/window function"}],"href":"/second-documentation/docs/category/data-analysis"}],"href":"/second-documentation/docs/category/pyspark"},{"type":"link","label":"Tutorial Intro","href":"/second-documentation/docs/intro","docId":"intro"}]},"docs":{"intro":{"id":"intro","title":"Tutorial Intro","description":"Let\'s discover Docusaurus in less than 5 minutes.","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/case when":{"id":"pyspark/Data Analysis/case when","title":"Case when","description":"","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/concat":{"id":"pyspark/Data Analysis/concat","title":"Concat (stack) dataframes","description":"\u0110\u1ec3 concat nhi\u1ec1u dataframe, ta c\xf3 th\u1ec3 d\xf9ng utility function wrap pyspark unionAll method","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/Datetime":{"id":"pyspark/Data Analysis/Datetime","title":"Datetime functions","description":"C\xf3 2 d\u1ea1ng bi\u1ebfn \u0111\u1ed5i c\u0103n b\u1ea3n c\u1ee7a datetime object: T\u1eeb string -> datetime v\xe0 t\u1eeb datetime -> formatted string","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/Explode row":{"id":"pyspark/Data Analysis/Explode row","title":"Explode row","description":"Explode row l\xe0 ph\u01b0\u01a1ng th\u1ee9c chuy\u1ec3n 1 Array Struct, ho\u1eb7c 1 cell c\xf3 ch\u1ee9a string with separator th\xe0nh nhi\u1ec1u rows","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/groupby and agg":{"id":"pyspark/Data Analysis/groupby and agg","title":"Groupby and Aggregation","description":"C\xf3 2 c\xe1ch s\u1eed d\u1ee5ng aggregation l\xe0 g\u1ecdi tr\u1ef1c ti\u1ebfp method (with only one agg function) ho\u1eb7c d\xf9ng .agg() method","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/join":{"id":"pyspark/Data Analysis/join","title":"Join","description":"Join c\u1ee7a pyspark kh\xe1 gi\u1ed1ng merge c\u1ee7a pandas.","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/melt":{"id":"pyspark/Data Analysis/melt","title":"Melt dataframe","description":"Pyspark ko c\xf3 native melt method. N\u1ebfu s\u1eed d\u1ee5ng b\u1ea3n pyspark 3.2.0, c\xf3 th\u1ec3 transform n\xf3 th\xe0nh koalas dataframe, sau \u0111\xf3 melt v\xe0 chuy\u1ec3n l\u1ea1i pyspark dataframe","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/udf":{"id":"pyspark/Data Analysis/udf","title":"User defined function","description":"No external argument","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/window function":{"id":"pyspark/Data Analysis/window function","title":"Window function","description":"Window function l\xe0 c\xe1c action th\u1ef1c hi\u1ec7n row-wise thay v\xec column wise. Window function bao g\u1ed3m c\xe1c reader function (rank, denserank, rownumber) v\xe0 c\xe1c aggregation utilize c\xe1c reader n\xe0y","sidebar":"tutorialSidebar"},"pyspark/Install pyspark":{"id":"pyspark/Install pyspark","title":"Install Pyspark","description":"1. Install JDK","sidebar":"tutorialSidebar"},"pyspark/M\xf4 h\xecnh Spark":{"id":"pyspark/M\xf4 h\xecnh Spark","title":"M\xf4 h\xecnh ho\u1ea1t \u0111\u1ed9ng c\u1ee7a pyspark","description":"mohinhhoatdongcuapyspark","sidebar":"tutorialSidebar"},"pyspark/Quy t\u1eafc x\u1eed l\xed d\u1eef li\u1ec7u":{"id":"pyspark/Quy t\u1eafc x\u1eed l\xed d\u1eef li\u1ec7u","title":"Quy t\u1eafc x\u1eed l\xed d\u1eef li\u1ec7u","description":"DataFrame c\u1ee7a pyspark l\xe0 immutable. Do \u0111\xf3, khi ta th\u1ef1c hi\u1ec7n 1 mutation, ta ph\u1ea3i ghi \u0111\xe8 l\xean bi\u1ebfn hi\u1ec7n t\u1ea1i. L\xfac n\xe0y, qu\xe1 tr\xecnh debug r\u1ea5t l\xe0 kh\xf3. Do \u0111\xf3, ta ph\u1ea3i c\xf3 1 b\u1ed9 quy t\u1eafc, khi n\xe0o t\u1ea1o bi\u1ebfn m\u1edbi, khi n\xe0o ghi \u0111\xe8, khi n\xe0o t\u1ea1o columns m\u1edbi ...","sidebar":"tutorialSidebar"},"pyspark/RDD v\xe0 DataFrame":{"id":"pyspark/RDD v\xe0 DataFrame","title":"RDD v\xe0 DataFrame","description":"Resilient Distributed Dataset l\xe0 th\xe0nh ph\u1ea7n core nh\u1ea5t c\u1ee7a spark. N\xf3 g\u1ed3m c\xe1c t\xednh ch\u1ea5t sau","sidebar":"tutorialSidebar"}}}')}}]);