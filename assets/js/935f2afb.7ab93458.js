"use strict";(self.webpackChunkcode_docs=self.webpackChunkcode_docs||[]).push([[53],{1109:a=>{a.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"SQL","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Th\u1ee9 t\u1ef1 th\u1ef1c thi c\xe2u l\u1ec7nh SQL","href":"/second-documentation/docs/sql/Execution order","docId":"sql/Execution order"},{"type":"link","label":"Correlated subquery","href":"/second-documentation/docs/sql/Correlated subquery","docId":"sql/Correlated subquery"},{"type":"link","label":"Delete rows","href":"/second-documentation/docs/sql/Delete row","docId":"sql/Delete row"},{"type":"link","label":"Insert rows","href":"/second-documentation/docs/sql/Insert row","docId":"sql/Insert row"},{"type":"link","label":"Update rows","href":"/second-documentation/docs/sql/Update row","docId":"sql/Update row"}],"href":"/second-documentation/docs/category/sql"},{"type":"category","label":"Pyspark","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Install Pyspark","href":"/second-documentation/docs/pyspark/Install pyspark","docId":"pyspark/Install pyspark"},{"type":"link","label":"M\xf4 h\xecnh ho\u1ea1t \u0111\u1ed9ng c\u1ee7a pyspark","href":"/second-documentation/docs/pyspark/M\xf4 h\xecnh Spark","docId":"pyspark/M\xf4 h\xecnh Spark"},{"type":"link","label":"RDD v\xe0 DataFrame","href":"/second-documentation/docs/pyspark/RDD v\xe0 DataFrame","docId":"pyspark/RDD v\xe0 DataFrame"},{"type":"link","label":"Quy t\u1eafc x\u1eed l\xed d\u1eef li\u1ec7u","href":"/second-documentation/docs/pyspark/Quy t\u1eafc x\u1eed l\xed d\u1eef li\u1ec7u","docId":"pyspark/Quy t\u1eafc x\u1eed l\xed d\u1eef li\u1ec7u"},{"type":"category","label":"Data Analysis","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Datetime functions","href":"/second-documentation/docs/pyspark/Data Analysis/Datetime","docId":"pyspark/Data Analysis/Datetime"},{"type":"link","label":"Duplicate values","href":"/second-documentation/docs/pyspark/Data Analysis/Duplicate values","docId":"pyspark/Data Analysis/Duplicate values"},{"type":"link","label":"Explode row","href":"/second-documentation/docs/pyspark/Data Analysis/Explode row","docId":"pyspark/Data Analysis/Explode row"},{"type":"link","label":"Fill missing data from row","href":"/second-documentation/docs/pyspark/Data Analysis/Foward fill","docId":"pyspark/Data Analysis/Foward fill"},{"type":"link","label":"Case when","href":"/second-documentation/docs/pyspark/Data Analysis/case when","docId":"pyspark/Data Analysis/case when"},{"type":"link","label":"Concat (stack) dataframes","href":"/second-documentation/docs/pyspark/Data Analysis/concat","docId":"pyspark/Data Analysis/concat"},{"type":"link","label":"Groupby and Aggregation","href":"/second-documentation/docs/pyspark/Data Analysis/groupby and agg","docId":"pyspark/Data Analysis/groupby and agg"},{"type":"link","label":"Join","href":"/second-documentation/docs/pyspark/Data Analysis/join","docId":"pyspark/Data Analysis/join"},{"type":"link","label":"Melt dataframe","href":"/second-documentation/docs/pyspark/Data Analysis/melt","docId":"pyspark/Data Analysis/melt"},{"type":"link","label":"User defined function","href":"/second-documentation/docs/pyspark/Data Analysis/udf","docId":"pyspark/Data Analysis/udf"},{"type":"link","label":"Window function","href":"/second-documentation/docs/pyspark/Data Analysis/window function","docId":"pyspark/Data Analysis/window function"}],"href":"/second-documentation/docs/category/data-analysis"}],"href":"/second-documentation/docs/category/pyspark"},{"type":"category","label":"GraphQL","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Create GraphQL server with Strapi","href":"/second-documentation/docs/graphql/With Strapi","docId":"graphql/With Strapi"}],"href":"/second-documentation/docs/category/graphql"},{"type":"link","label":"Tutorial Intro","href":"/second-documentation/docs/intro","docId":"intro"}]},"docs":{"graphql/With Strapi":{"id":"graphql/With Strapi","title":"Create GraphQL server with Strapi","description":"Reference 1,","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Tutorial Intro","description":"Let\'s discover Docusaurus in less than 5 minutes.","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/case when":{"id":"pyspark/Data Analysis/case when","title":"Case when","description":"","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/concat":{"id":"pyspark/Data Analysis/concat","title":"Concat (stack) dataframes","description":"\u0110\u1ec3 concat nhi\u1ec1u dataframe, ta c\xf3 th\u1ec3 d\xf9ng utility function wrap pyspark unionAll method","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/Datetime":{"id":"pyspark/Data Analysis/Datetime","title":"Datetime functions","description":"C\xf3 2 d\u1ea1ng bi\u1ebfn \u0111\u1ed5i c\u0103n b\u1ea3n c\u1ee7a datetime object: T\u1eeb string -> datetime v\xe0 t\u1eeb datetime -> formatted string","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/Duplicate values":{"id":"pyspark/Data Analysis/Duplicate values","title":"Duplicate values","description":"Find duplicate values","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/Explode row":{"id":"pyspark/Data Analysis/Explode row","title":"Explode row","description":"Explode row l\xe0 ph\u01b0\u01a1ng th\u1ee9c chuy\u1ec3n 1 Array Struct, ho\u1eb7c 1 cell c\xf3 ch\u1ee9a string with separator th\xe0nh nhi\u1ec1u rows","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/Foward fill":{"id":"pyspark/Data Analysis/Foward fill","title":"Fill missing data from row","description":"Ta c\xf3 th\u1ec3 fill c\xe1c d\u1eef li\u1ec7u null t\u1eeb c\xe1c d\xf2ng \u0111\u1ee9ng tr\u01b0\u1edbc ho\u1eb7c \u0111\u1ee9ng sau n\xf3.","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/groupby and agg":{"id":"pyspark/Data Analysis/groupby and agg","title":"Groupby and Aggregation","description":"C\xf3 2 c\xe1ch s\u1eed d\u1ee5ng aggregation l\xe0 g\u1ecdi tr\u1ef1c ti\u1ebfp method (with only one agg function) ho\u1eb7c d\xf9ng .agg() method","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/join":{"id":"pyspark/Data Analysis/join","title":"Join","description":"Join c\u1ee7a pyspark kh\xe1 gi\u1ed1ng merge c\u1ee7a pandas.","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/melt":{"id":"pyspark/Data Analysis/melt","title":"Melt dataframe","description":"Pyspark ko c\xf3 native melt method. N\u1ebfu s\u1eed d\u1ee5ng b\u1ea3n pyspark 3.2.0, c\xf3 th\u1ec3 transform n\xf3 th\xe0nh koalas dataframe, sau \u0111\xf3 melt v\xe0 chuy\u1ec3n l\u1ea1i pyspark dataframe","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/udf":{"id":"pyspark/Data Analysis/udf","title":"User defined function","description":"No external argument","sidebar":"tutorialSidebar"},"pyspark/Data Analysis/window function":{"id":"pyspark/Data Analysis/window function","title":"Window function","description":"Window function l\xe0 c\xe1c action th\u1ef1c hi\u1ec7n row-wise thay v\xec column wise. Window function bao g\u1ed3m c\xe1c reader function (rank, denserank, rownumber) v\xe0 c\xe1c aggregation utilize c\xe1c reader n\xe0y","sidebar":"tutorialSidebar"},"pyspark/Install pyspark":{"id":"pyspark/Install pyspark","title":"Install Pyspark","description":"1. Install JDK","sidebar":"tutorialSidebar"},"pyspark/M\xf4 h\xecnh Spark":{"id":"pyspark/M\xf4 h\xecnh Spark","title":"M\xf4 h\xecnh ho\u1ea1t \u0111\u1ed9ng c\u1ee7a pyspark","description":"mohinhhoatdongcuapyspark","sidebar":"tutorialSidebar"},"pyspark/Quy t\u1eafc x\u1eed l\xed d\u1eef li\u1ec7u":{"id":"pyspark/Quy t\u1eafc x\u1eed l\xed d\u1eef li\u1ec7u","title":"Quy t\u1eafc x\u1eed l\xed d\u1eef li\u1ec7u","description":"DataFrame c\u1ee7a pyspark l\xe0 immutable. Do \u0111\xf3, khi ta th\u1ef1c hi\u1ec7n 1 mutation, ta ph\u1ea3i ghi \u0111\xe8 l\xean bi\u1ebfn hi\u1ec7n t\u1ea1i. L\xfac n\xe0y, qu\xe1 tr\xecnh debug r\u1ea5t l\xe0 kh\xf3. Do \u0111\xf3, ta ph\u1ea3i c\xf3 1 b\u1ed9 quy t\u1eafc, khi n\xe0o t\u1ea1o bi\u1ebfn m\u1edbi, khi n\xe0o ghi \u0111\xe8, khi n\xe0o t\u1ea1o columns m\u1edbi ...","sidebar":"tutorialSidebar"},"pyspark/RDD v\xe0 DataFrame":{"id":"pyspark/RDD v\xe0 DataFrame","title":"RDD v\xe0 DataFrame","description":"Resilient Distributed Dataset l\xe0 th\xe0nh ph\u1ea7n core nh\u1ea5t c\u1ee7a spark. N\xf3 g\u1ed3m c\xe1c t\xednh ch\u1ea5t sau","sidebar":"tutorialSidebar"},"sql/Correlated subquery":{"id":"sql/Correlated subquery","title":"Correlated subquery","description":"Correlated subquery l\xe0 1 query \u0111\u01b0\u1ee3c trigger nhi\u1ec1u h\u01a1n 1 l\u1ea7n b\u1edfi outer query.","sidebar":"tutorialSidebar"},"sql/Delete row":{"id":"sql/Delete row","title":"Delete rows","description":"X\xf3a c\xe1c d\xf2ng m\xe0 n\xf3 kh\xf4ng t\u1ed3n t\u1ea1i trong b\u1ea3ng kh\xe1c","sidebar":"tutorialSidebar"},"sql/Execution order":{"id":"sql/Execution order","title":"Th\u1ee9 t\u1ef1 th\u1ef1c thi c\xe2u l\u1ec7nh SQL","description":"1. FROM: Tr\u1ea3 v\u1ec1 c\xe1c rows th\u1ecfa \u0111i\u1ec1u ki\u1ec7n ON c\u1ee7a JOIN ho\u1eb7c t\u1eeb subqueries","sidebar":"tutorialSidebar"},"sql/Insert row":{"id":"sql/Insert row","title":"Insert rows","description":"Insert row with value from an aggregation","sidebar":"tutorialSidebar"},"sql/Update row":{"id":"sql/Update row","title":"Update rows","description":"Update rows from a subquery","sidebar":"tutorialSidebar"}}}')}}]);